{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNLZOGTpll-"
      },
      "source": [
        "### Install requirement\n",
        "\n",
        "Reference: https://github.com/NVlabs/stylegan2-ada-pytorch/issues/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-yNo3MWpsV8",
        "outputId": "a9a86efb-152f-454d-c383-fac190152f60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%pip install ninja"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/422.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/422.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting Drive"
      ],
      "metadata": {
        "id": "CF3cttptCdxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_Cq-WxBCf7n",
        "outputId": "f4c7e06c-d019-4207-ee7f-926a6eee495b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0oZRRs2KO5A"
      },
      "source": [
        "## Clone StyleGAN\n",
        "###Only do once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKYZ49zf41Sv",
        "outputId": "d19947a2-81eb-483e-ec63-ca54a761a2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "%rm -rf stylegan2-ada-pytorch/\n",
        "!git clone https://github.com/woctezuma/stylegan2-ada-pytorch.git\n",
        "\n",
        "%cd stylegan2-ada-pytorch/\n",
        "!git checkout google-colab\n",
        "\n",
        "%cd /content\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 182, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 182 (delta 18), reused 16 (delta 16), pack-reused 159 (from 3)\u001b[K\n",
            "Receiving objects: 100% (182/182), 1.14 MiB | 2.52 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "/content/drive/MyDrive/stylegan2-ada-pytorch\n",
            "M\t.gitignore\n",
            "M\tDockerfile\n",
            "M\tLICENSE.txt\n",
            "M\tREADME.md\n",
            "M\tcalc_metrics.py\n",
            "M\tdataset_tool.py\n",
            "M\tdnnlib/__init__.py\n",
            "M\tdnnlib/util.py\n",
            "M\tdocker_run.sh\n",
            "M\tdocs/dataset-tool-help.txt\n",
            "M\tdocs/license.html\n",
            "M\tdocs/stylegan2-ada-teaser-1024x252.png\n",
            "M\tdocs/stylegan2-ada-training-curves.png\n",
            "M\tdocs/train-help.txt\n",
            "M\tgenerate.py\n",
            "M\tlegacy.py\n",
            "M\tmetrics/__init__.py\n",
            "M\tmetrics/frechet_inception_distance.py\n",
            "M\tmetrics/inception_score.py\n",
            "M\tmetrics/kernel_inception_distance.py\n",
            "M\tmetrics/metric_main.py\n",
            "M\tmetrics/metric_utils.py\n",
            "M\tmetrics/perceptual_path_length.py\n",
            "M\tmetrics/precision_recall.py\n",
            "M\tprojector.py\n",
            "M\tstyle_mixing.py\n",
            "M\ttorch_utils/__init__.py\n",
            "M\ttorch_utils/custom_ops.py\n",
            "M\ttorch_utils/misc.py\n",
            "M\ttorch_utils/ops/__init__.py\n",
            "M\ttorch_utils/ops/bias_act.cpp\n",
            "M\ttorch_utils/ops/bias_act.cu\n",
            "M\ttorch_utils/ops/bias_act.h\n",
            "M\ttorch_utils/ops/bias_act.py\n",
            "M\ttorch_utils/ops/conv2d_gradfix.py\n",
            "M\ttorch_utils/ops/conv2d_resample.py\n",
            "M\ttorch_utils/ops/fma.py\n",
            "M\ttorch_utils/ops/grid_sample_gradfix.py\n",
            "M\ttorch_utils/ops/upfirdn2d.cpp\n",
            "M\ttorch_utils/ops/upfirdn2d.cu\n",
            "M\ttorch_utils/ops/upfirdn2d.h\n",
            "M\ttorch_utils/ops/upfirdn2d.py\n",
            "M\ttorch_utils/persistence.py\n",
            "M\ttorch_utils/training_stats.py\n",
            "M\ttrain.py\n",
            "M\ttraining/__init__.py\n",
            "M\ttraining/augment.py\n",
            "M\ttraining/dataset.py\n",
            "M\ttraining/loss.py\n",
            "M\ttraining/networks.py\n",
            "M\ttraining/training_loop.py\n",
            "Already on 'google-colab'\n",
            "Your branch is up to date with 'origin/google-colab'.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Pokemon Dataset"
      ],
      "metadata": {
        "id": "P0sshPviCsCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1RvZbpemMp0Q0U7Y6Z7RphHUPdrd4gcEl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md3m1JQxCtml",
        "outputId": "1f27a400-0c0b-4cd6-ea2a-8ff883f081d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1RvZbpemMp0Q0U7Y6Z7RphHUPdrd4gcEl\n",
            "From (redirected): https://drive.google.com/uc?id=1RvZbpemMp0Q0U7Y6Z7RphHUPdrd4gcEl&confirm=t&uuid=195f1189-b328-4368-b9e1-de2510748f1c\n",
            "To: /content/pokemon.zip\n",
            "100% 41.4M/41.4M [00:00<00:00, 105MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augmentation"
      ],
      "metadata": {
        "id": "FBfD_Gb4-KlD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04SYpwjy8_5G",
        "outputId": "eaa2b859-d0d6-4a7f-cf55-41fb66389380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Default title text\n",
        "# Define dataset paths\n",
        "dataset_zip_path = \"/content/pokemon.zip\"\n",
        "extract_folder = \"/content/dataset\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "import zipfile  # Import zipfile module\n",
        "\n",
        "# Extract the dataset if it hasn't been already\n",
        "if not os.path.exists(os.path.join(extract_folder, \"pokemon\")):\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        print(f\"Extracting {dataset_zip_path} to {extract_folder}...\")\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    print(\"✅ Extraction complete!\")\n",
        "else:\n",
        "    print(\"✅ Dataset already extracted.\")\n",
        "\n",
        "# Set dataset paths\n",
        "dataset_path = \"/content/dataset\"\n",
        "input_folder = \"/content/dataset/pokemon\"  # Update based on actual structure\n",
        "output_folder = \"/content/dataset/augmented_images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Verify if images exist\n",
        "image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "if not image_files:\n",
        "    raise ValueError(f\"❌ No images found in {input_folder}. Check folder structure.\")\n",
        "else:\n",
        "    print(f\"✅ Found {len(image_files)} images in {input_folder}\")\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "# Define augmentation pipeline\n",
        "augmentations = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.2),\n",
        "    # A.RandomBrightnessContrast(p=0.3),\n",
        "    # A.Blur(blur_limit=3, p=0.2),\n",
        "    A.Rotate(limit=30, p=0.5),\n",
        "    # A.GaussNoise(var_limit=50.0, p=0.3),  # Fixed argument\n",
        "    # A.RandomCrop(height=64, width=64, p=1.0)\n",
        "])\n",
        "\n",
        "# Determine number of copies needed (prevent ZeroDivisionError)\n",
        "target_size = 10000\n",
        "num_copies = (target_size // max(len(image_files), 1)) + 1\n",
        "\n",
        "# Apply augmentations & save\n",
        "counter = 0\n",
        "for img_file in tqdm(image_files * num_copies):\n",
        "    img = cv2.imread(os.path.join(input_folder, img_file))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    augmented = augmentations(image=img)[\"image\"]\n",
        "    save_path = os.path.join(output_folder, f\"aug_{counter}.png\")\n",
        "    cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "    counter += 1\n",
        "    if counter >= target_size:\n",
        "        break  # Stop once we have enough images\n",
        "\n",
        "print(f\"✅ Dataset augmented to {counter} images!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/pokemon.zip to /content/dataset...\n",
            "✅ Extraction complete!\n",
            "✅ Found 819 images in /content/dataset/pokemon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 9999/10647 [01:01<00:03, 162.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset augmented to 10000 images!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo5KFL4yw0j"
      },
      "source": [
        "### Prepare datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/stylegan2-ada-pytorch/dataset_tool.py --source=/content/dataset/augmented_images --dest=/content/dataset/augmented_tfrecords --width=64 --height=64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c0_pALc8i6-",
        "outputId": "ca38616e-fb56-437b-9ac0-0649f8923323"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 10000/10000 [00:33<00:00, 297.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "1dgRP6nv-SwO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97U6mcsuoCiI"
      },
      "source": [
        "\n",
        "augmented_images_path = \"/content/dataset/augmented_images\"\n",
        "training_folder = \"/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For first time"
      ],
      "metadata": {
        "id": "SmcxI3WMH4mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/stylegan2-ada-pytorch/train.py --outdir={training_folder} --data /content/dataset/augmented_tfrecords --gpus=1 --batch=32 --kimg=5000 --mirror=1 --snap=10 --metrics=none  --cfg_map=8 \\\n",
        " --augpipe=bg \\\n",
        " --freezed=10 \\\n",
        " --resume=ffhq256"
      ],
      "metadata": {
        "id": "qqsmZh0b5XkO",
        "outputId": "474b1614-3c4c-4da1-af1d-3d4cea57388e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/dataset/augmented_tfrecords\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 10000,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 64\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 10\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.0256\n",
            "  },\n",
            "  \"total_kimg\": 5000,\n",
            "  \"batch_size\": 32,\n",
            "  \"batch_gpu\": 32,\n",
            "  \"ema_kimg\": 10.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/00000-augmented_tfrecords-mirror-auto1-kimg5000-batch32-bg-resumeffhq256-freezed10\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/00000-augmented_tfrecords-mirror-auto1-kimg5000-batch32-bg-resumeffhq256-freezed10\n",
            "Training data:      /content/dataset/augmented_tfrecords\n",
            "Training duration:  5000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   10000\n",
            "Image resolution:   64\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py:77: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "Num images:  20000\n",
            "Image shape: [3, 64, 64]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl\"\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl ... done\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "\n",
            "Generator            Parameters  Buffers  Output shape       Datatype\n",
            "---                  ---         ---      ---                ---     \n",
            "mapping.fc0          262656      -        [32, 512]          float32 \n",
            "mapping.fc1          262656      -        [32, 512]          float32 \n",
            "mapping.fc2          262656      -        [32, 512]          float32 \n",
            "mapping.fc3          262656      -        [32, 512]          float32 \n",
            "mapping.fc4          262656      -        [32, 512]          float32 \n",
            "mapping.fc5          262656      -        [32, 512]          float32 \n",
            "mapping.fc6          262656      -        [32, 512]          float32 \n",
            "mapping.fc7          262656      -        [32, 512]          float32 \n",
            "mapping              -           512      [32, 10, 512]      float32 \n",
            "synthesis.b4.conv1   2622465     32       [32, 512, 4, 4]    float32 \n",
            "synthesis.b4.torgb   264195      -        [32, 3, 4, 4]      float32 \n",
            "synthesis.b4:0       8192        16       [32, 512, 4, 4]    float32 \n",
            "synthesis.b4:1       -           -        [32, 512, 4, 4]    float32 \n",
            "synthesis.b8.conv0   2622465     80       [32, 512, 8, 8]    float16 \n",
            "synthesis.b8.conv1   2622465     80       [32, 512, 8, 8]    float16 \n",
            "synthesis.b8.torgb   264195      -        [32, 3, 8, 8]      float16 \n",
            "synthesis.b8:0       -           16       [32, 512, 8, 8]    float16 \n",
            "synthesis.b8:1       -           -        [32, 512, 8, 8]    float32 \n",
            "synthesis.b16.conv0  2622465     272      [32, 512, 16, 16]  float16 \n",
            "synthesis.b16.conv1  2622465     272      [32, 512, 16, 16]  float16 \n",
            "synthesis.b16.torgb  264195      -        [32, 3, 16, 16]    float16 \n",
            "synthesis.b16:0      -           16       [32, 512, 16, 16]  float16 \n",
            "synthesis.b16:1      -           -        [32, 512, 16, 16]  float32 \n",
            "synthesis.b32.conv0  2622465     1040     [32, 512, 32, 32]  float16 \n",
            "synthesis.b32.conv1  2622465     1040     [32, 512, 32, 32]  float16 \n",
            "synthesis.b32.torgb  264195      -        [32, 3, 32, 32]    float16 \n",
            "synthesis.b32:0      -           16       [32, 512, 32, 32]  float16 \n",
            "synthesis.b32:1      -           -        [32, 512, 32, 32]  float32 \n",
            "synthesis.b64.conv0  1442561     4112     [32, 256, 64, 64]  float16 \n",
            "synthesis.b64.conv1  721409      4112     [32, 256, 64, 64]  float16 \n",
            "synthesis.b64.torgb  132099      -        [32, 3, 64, 64]    float16 \n",
            "synthesis.b64:0      -           16       [32, 256, 64, 64]  float16 \n",
            "synthesis.b64:1      -           -        [32, 256, 64, 64]  float32 \n",
            "---                  ---         ---      ---                ---     \n",
            "Total                23819544    11632    -                  -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers   Output shape       Datatype\n",
            "---            ---         ---       ---                ---     \n",
            "b64.fromrgb    -           1040      [32, 256, 64, 64]  float16 \n",
            "b64.skip       -           131088    [32, 512, 32, 32]  float16 \n",
            "b64.conv0      -           590096    [32, 256, 64, 64]  float16 \n",
            "b64.conv1      -           1180176   [32, 512, 32, 32]  float16 \n",
            "b64            -           16        [32, 512, 32, 32]  float16 \n",
            "b32.skip       -           262160    [32, 512, 16, 16]  float16 \n",
            "b32.conv0      -           2359824   [32, 512, 32, 32]  float16 \n",
            "b32.conv1      -           2359824   [32, 512, 16, 16]  float16 \n",
            "b32            -           16        [32, 512, 16, 16]  float16 \n",
            "b16.skip       -           262160    [32, 512, 8, 8]    float16 \n",
            "b16.conv0      -           2359824   [32, 512, 16, 16]  float16 \n",
            "b16.conv1      -           2359824   [32, 512, 8, 8]    float16 \n",
            "b16            -           16        [32, 512, 8, 8]    float16 \n",
            "b8.skip        262144      16        [32, 512, 4, 4]    float16 \n",
            "b8.conv0       2359808     16        [32, 512, 8, 8]    float16 \n",
            "b8.conv1       2359808     16        [32, 512, 4, 4]    float16 \n",
            "b8             -           16        [32, 512, 4, 4]    float16 \n",
            "b4.mbstd       -           -         [32, 513, 4, 4]    float32 \n",
            "b4.conv        2364416     16        [32, 512, 4, 4]    float32 \n",
            "b4.fc          4194816     -         [32, 512]          float32 \n",
            "b4.out         513         -         [32, 1]            float32 \n",
            "---            ---         ---       ---                ---     \n",
            "Total          11541505    11866144  -                  -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2025-03-23 12:05:50.418925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742731550.437304    1907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742731550.442632    1907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 12:05:50.460632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Training for 5000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 40s       sec/tick 7.9     sec/kimg 245.69  maintenance 92.0   cpumem 2.75   gpumem 9.09   augment 0.000\n",
            "tick 1     kimg 4.0      time 3m 09s       sec/tick 84.8    sec/kimg 21.20   maintenance 4.8    cpumem 3.02   gpumem 5.15   augment 0.008\n",
            "tick 2     kimg 8.0      time 4m 36s       sec/tick 86.4    sec/kimg 21.61   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.004\n",
            "tick 3     kimg 12.0     time 6m 03s       sec/tick 87.5    sec/kimg 21.87   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 4     kimg 16.0     time 7m 32s       sec/tick 88.3    sec/kimg 22.08   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.001\n",
            "tick 5     kimg 20.0     time 9m 00s       sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 6     kimg 24.0     time 10m 28s      sec/tick 87.9    sec/kimg 21.97   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 7     kimg 28.0     time 11m 56s      sec/tick 88.2    sec/kimg 22.05   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 8     kimg 32.0     time 13m 25s      sec/tick 88.5    sec/kimg 22.11   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 9     kimg 36.0     time 14m 53s      sec/tick 88.1    sec/kimg 22.03   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 10    kimg 40.0     time 16m 21s      sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 11    kimg 44.0     time 17m 53s      sec/tick 88.0    sec/kimg 22.00   maintenance 4.5    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 12    kimg 48.0     time 19m 22s      sec/tick 88.5    sec/kimg 22.13   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 13    kimg 52.0     time 20m 50s      sec/tick 88.0    sec/kimg 22.01   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 14    kimg 56.0     time 22m 18s      sec/tick 88.1    sec/kimg 22.03   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 15    kimg 60.0     time 23m 46s      sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.001\n",
            "tick 16    kimg 64.0     time 25m 15s      sec/tick 88.3    sec/kimg 22.07   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 17    kimg 68.0     time 26m 43s      sec/tick 87.8    sec/kimg 21.95   maintenance 0.2    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 18    kimg 72.0     time 28m 11s      sec/tick 88.0    sec/kimg 22.00   maintenance 0.1    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 19    kimg 76.0     time 29m 39s      sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.005\n",
            "tick 20    kimg 80.0     time 31m 07s      sec/tick 88.5    sec/kimg 22.12   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.000\n",
            "tick 21    kimg 84.0     time 32m 43s      sec/tick 88.2    sec/kimg 22.05   maintenance 7.7    cpumem 2.89   gpumem 5.15   augment 0.001\n",
            "tick 22    kimg 88.0     time 34m 11s      sec/tick 87.8    sec/kimg 21.96   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.005\n",
            "tick 23    kimg 92.0     time 35m 39s      sec/tick 88.2    sec/kimg 22.06   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.000\n",
            "tick 24    kimg 96.0     time 37m 08s      sec/tick 88.5    sec/kimg 22.11   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.003\n",
            "tick 25    kimg 100.0    time 38m 36s      sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.000\n",
            "tick 26    kimg 104.0    time 40m 04s      sec/tick 88.0    sec/kimg 22.00   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.000\n",
            "tick 27    kimg 108.0    time 41m 32s      sec/tick 87.8    sec/kimg 21.95   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.005\n",
            "tick 28    kimg 112.0    time 43m 00s      sec/tick 88.4    sec/kimg 22.11   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.012\n",
            "tick 29    kimg 116.0    time 44m 29s      sec/tick 88.1    sec/kimg 22.04   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.022\n",
            "tick 30    kimg 120.0    time 45m 57s      sec/tick 88.0    sec/kimg 22.01   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.015\n",
            "tick 31    kimg 124.0    time 47m 35s      sec/tick 88.3    sec/kimg 22.08   maintenance 9.7    cpumem 3.02   gpumem 5.15   augment 0.020\n",
            "tick 32    kimg 128.0    time 49m 03s      sec/tick 88.4    sec/kimg 22.11   maintenance 0.0    cpumem 2.95   gpumem 5.15   augment 0.024\n",
            "tick 33    kimg 132.0    time 50m 31s      sec/tick 87.8    sec/kimg 21.96   maintenance 0.2    cpumem 2.95   gpumem 5.15   augment 0.028\n",
            "tick 34    kimg 136.0    time 51m 59s      sec/tick 88.2    sec/kimg 22.05   maintenance 0.0    cpumem 2.95   gpumem 5.15   augment 0.020\n",
            "tick 35    kimg 140.0    time 53m 27s      sec/tick 88.1    sec/kimg 22.03   maintenance 0.1    cpumem 2.95   gpumem 5.15   augment 0.032\n",
            "tick 36    kimg 144.0    time 54m 56s      sec/tick 88.5    sec/kimg 22.14   maintenance 0.0    cpumem 2.95   gpumem 5.15   augment 0.038\n",
            "tick 37    kimg 148.0    time 56m 24s      sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 2.95   gpumem 5.15   augment 0.045\n",
            "tick 38    kimg 152.0    time 57m 52s      sec/tick 87.9    sec/kimg 21.96   maintenance 0.0    cpumem 2.95   gpumem 5.15   augment 0.056\n",
            "tick 39    kimg 156.0    time 59m 20s      sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.056\n",
            "tick 40    kimg 160.0    time 1h 00m 49s   sec/tick 88.6    sec/kimg 22.15   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.061\n",
            "tick 41    kimg 164.0    time 1h 02m 26s   sec/tick 88.3    sec/kimg 22.08   maintenance 8.8    cpumem 2.89   gpumem 5.15   augment 0.063\n",
            "tick 42    kimg 168.0    time 1h 03m 54s   sec/tick 88.2    sec/kimg 22.06   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.061\n",
            "tick 43    kimg 172.0    time 1h 05m 22s   sec/tick 87.9    sec/kimg 21.97   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.061\n",
            "tick 44    kimg 176.0    time 1h 06m 51s   sec/tick 88.5    sec/kimg 22.12   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.063\n",
            "tick 45    kimg 180.0    time 1h 08m 19s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.064\n",
            "tick 46    kimg 184.0    time 1h 09m 47s   sec/tick 88.1    sec/kimg 22.03   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.068\n",
            "tick 47    kimg 188.0    time 1h 11m 15s   sec/tick 88.3    sec/kimg 22.07   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.072\n",
            "tick 48    kimg 192.0    time 1h 12m 44s   sec/tick 88.6    sec/kimg 22.14   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.073\n",
            "tick 49    kimg 196.0    time 1h 14m 12s   sec/tick 88.2    sec/kimg 22.05   maintenance 0.2    cpumem 2.89   gpumem 5.15   augment 0.082\n",
            "tick 50    kimg 200.0    time 1h 15m 41s   sec/tick 88.4    sec/kimg 22.09   maintenance 0.0    cpumem 2.89   gpumem 5.15   augment 0.078\n",
            "tick 51    kimg 204.0    time 1h 17m 14s   sec/tick 88.5    sec/kimg 22.13   maintenance 5.1    cpumem 3.02   gpumem 5.15   augment 0.069\n",
            "tick 52    kimg 208.0    time 1h 18m 43s   sec/tick 88.8    sec/kimg 22.19   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.078\n",
            "tick 53    kimg 212.0    time 1h 20m 11s   sec/tick 88.3    sec/kimg 22.08   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.087\n",
            "tick 54    kimg 216.0    time 1h 21m 39s   sec/tick 88.0    sec/kimg 22.01   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.088\n",
            "tick 55    kimg 220.0    time 1h 23m 08s   sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.088\n",
            "tick 56    kimg 224.0    time 1h 24m 36s   sec/tick 88.5    sec/kimg 22.13   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.092\n",
            "tick 57    kimg 228.0    time 1h 26m 05s   sec/tick 88.3    sec/kimg 22.06   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.095\n",
            "tick 58    kimg 232.0    time 1h 27m 33s   sec/tick 88.3    sec/kimg 22.08   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.093\n",
            "tick 59    kimg 236.0    time 1h 29m 01s   sec/tick 88.1    sec/kimg 22.01   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.101\n",
            "tick 60    kimg 240.0    time 1h 30m 30s   sec/tick 88.6    sec/kimg 22.14   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.091\n",
            "tick 61    kimg 244.0    time 1h 32m 06s   sec/tick 88.3    sec/kimg 22.07   maintenance 8.5    cpumem 3.02   gpumem 5.15   augment 0.087\n",
            "tick 62    kimg 248.0    time 1h 33m 35s   sec/tick 88.1    sec/kimg 22.03   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.093\n",
            "tick 63    kimg 252.0    time 1h 35m 03s   sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.095\n",
            "tick 64    kimg 256.0    time 1h 36m 31s   sec/tick 88.4    sec/kimg 22.10   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.092\n",
            "tick 65    kimg 260.0    time 1h 37m 59s   sec/tick 87.9    sec/kimg 21.98   maintenance 0.2    cpumem 3.02   gpumem 5.15   augment 0.104\n",
            "tick 66    kimg 264.0    time 1h 39m 27s   sec/tick 88.1    sec/kimg 22.01   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.102\n",
            "tick 67    kimg 268.0    time 1h 40m 56s   sec/tick 88.1    sec/kimg 22.03   maintenance 0.1    cpumem 3.02   gpumem 5.15   augment 0.099\n",
            "tick 68    kimg 272.0    time 1h 42m 24s   sec/tick 88.6    sec/kimg 22.14   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.097\n",
            "tick 69    kimg 276.0    time 1h 43m 52s   sec/tick 88.1    sec/kimg 22.03   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.093\n",
            "tick 70    kimg 280.0    time 1h 45m 20s   sec/tick 87.9    sec/kimg 21.97   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.099\n",
            "tick 71    kimg 284.0    time 1h 46m 53s   sec/tick 88.3    sec/kimg 22.07   maintenance 4.7    cpumem 3.02   gpumem 5.15   augment 0.099\n",
            "tick 72    kimg 288.0    time 1h 48m 22s   sec/tick 88.6    sec/kimg 22.14   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.097\n",
            "tick 73    kimg 292.0    time 1h 49m 50s   sec/tick 88.0    sec/kimg 21.99   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.099\n",
            "tick 74    kimg 296.0    time 1h 51m 18s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.102\n",
            "tick 75    kimg 300.0    time 1h 52m 46s   sec/tick 88.0    sec/kimg 21.99   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.092\n",
            "tick 76    kimg 304.0    time 1h 54m 14s   sec/tick 88.5    sec/kimg 22.13   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.096\n",
            "tick 77    kimg 308.0    time 1h 55m 43s   sec/tick 88.1    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.102\n",
            "tick 78    kimg 312.0    time 1h 57m 11s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.100\n",
            "tick 79    kimg 316.0    time 1h 58m 39s   sec/tick 88.1    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.100\n",
            "tick 80    kimg 320.0    time 2h 00m 07s   sec/tick 88.5    sec/kimg 22.11   maintenance 0.0    cpumem 2.76   gpumem 5.15   augment 0.101\n",
            "tick 81    kimg 324.0    time 2h 01m 40s   sec/tick 88.0    sec/kimg 22.01   maintenance 4.6    cpumem 3.02   gpumem 5.15   augment 0.100\n",
            "tick 82    kimg 328.0    time 2h 03m 08s   sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.101\n",
            "tick 83    kimg 332.0    time 2h 04m 36s   sec/tick 88.2    sec/kimg 22.05   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.108\n",
            "tick 84    kimg 336.0    time 2h 06m 05s   sec/tick 88.5    sec/kimg 22.12   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.118\n",
            "tick 85    kimg 340.0    time 2h 07m 33s   sec/tick 88.2    sec/kimg 22.04   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.114\n",
            "tick 86    kimg 344.0    time 2h 09m 01s   sec/tick 87.8    sec/kimg 21.95   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.120\n",
            "tick 87    kimg 348.0    time 2h 10m 29s   sec/tick 88.2    sec/kimg 22.05   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.120\n",
            "tick 88    kimg 352.0    time 2h 11m 58s   sec/tick 88.4    sec/kimg 22.11   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.111\n",
            "tick 89    kimg 356.0    time 2h 13m 26s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.118\n",
            "tick 90    kimg 360.0    time 2h 14m 54s   sec/tick 88.1    sec/kimg 22.01   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.115\n",
            "tick 91    kimg 364.0    time 2h 16m 27s   sec/tick 88.0    sec/kimg 22.00   maintenance 4.6    cpumem 3.02   gpumem 5.15   augment 0.115\n",
            "tick 92    kimg 368.0    time 2h 17m 55s   sec/tick 88.4    sec/kimg 22.11   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.114\n",
            "tick 93    kimg 372.0    time 2h 19m 23s   sec/tick 88.1    sec/kimg 22.04   maintenance 0.1    cpumem 3.02   gpumem 5.15   augment 0.122\n",
            "tick 94    kimg 376.0    time 2h 20m 51s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.127\n",
            "tick 95    kimg 380.0    time 2h 22m 19s   sec/tick 88.1    sec/kimg 22.02   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.119\n",
            "tick 96    kimg 384.0    time 2h 23m 48s   sec/tick 88.4    sec/kimg 22.09   maintenance 0.0    cpumem 3.02   gpumem 5.15   augment 0.115\n",
            "tick 97    kimg 388.0    time 2h 25m 16s   sec/tick 87.9    sec/kimg 21.98   maintenance 0.2    cpumem 3.02   gpumem 5.15   augment 0.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/stylegan2-ada-pytorch/train.py --outdir={training_folder} --data /content/dataset/augmented_tfrecords --gpus=1 --batch=32 --kimg=5000 --mirror=1 --snap=10 --metrics=none  --cfg_map=8 \\\n",
        " --augpipe=bg \\\n",
        " --freezed=10 \\\n",
        " --resume=latest"
      ],
      "metadata": {
        "id": "Eeo0pNNJAuno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84fcdcf-7220-42ff-c4c2-392697c98589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/dataset/augmented_images\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 10000,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 10\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 5000,\n",
            "  \"batch_size\": 32,\n",
            "  \"batch_gpu\": 32,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"latest\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/00002-augmented_images-mirror-auto1-kimg5000-batch32-bg-resumecustom-freezed10\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/00002-augmented_images-mirror-auto1-kimg5000-batch32-bg-resumecustom-freezed10\n",
            "Training data:      /content/dataset/augmented_images\n",
            "Training duration:  5000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   10000\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py:77: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "Num images:  20000\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming with augment_p = 0.114\n",
            "Resuming from \"/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/00001-augmented_images-mirror-auto1-kimg5000-batch32-bg-resumecustom-freezed10/network-snapshot-000000.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [32, 512]            float32 \n",
            "mapping.fc1           262656      -        [32, 512]            float32 \n",
            "mapping.fc2           262656      -        [32, 512]            float32 \n",
            "mapping.fc3           262656      -        [32, 512]            float32 \n",
            "mapping.fc4           262656      -        [32, 512]            float32 \n",
            "mapping.fc5           262656      -        [32, 512]            float32 \n",
            "mapping.fc6           262656      -        [32, 512]            float32 \n",
            "mapping.fc7           262656      -        [32, 512]            float32 \n",
            "mapping               -           512      [32, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [32, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [32, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [32, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [32, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [32, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [32, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [32, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [32, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [32, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [32, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [32, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [32, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [32, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [32, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [32, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [32, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [32, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [32, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [32, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [32, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [32, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [32, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [32, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [32, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [32, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [32, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [32, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [32, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [32, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [32, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [32, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 24767458    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   -           272      [32, 64, 256, 256]   float16 \n",
            "b256.skip      -           8208     [32, 128, 128, 128]  float16 \n",
            "b256.conv0     -           36944    [32, 64, 256, 256]   float16 \n",
            "b256.conv1     -           73872    [32, 128, 128, 128]  float16 \n",
            "b256           -           16       [32, 128, 128, 128]  float16 \n",
            "b128.skip      -           32784    [32, 256, 64, 64]    float16 \n",
            "b128.conv0     -           147600   [32, 128, 128, 128]  float16 \n",
            "b128.conv1     -           295184   [32, 256, 64, 64]    float16 \n",
            "b128           -           16       [32, 256, 64, 64]    float16 \n",
            "b64.skip       -           131088   [32, 512, 32, 32]    float16 \n",
            "b64.conv0      -           590096   [32, 256, 64, 64]    float16 \n",
            "b64.conv1      -           1180176  [32, 512, 32, 32]    float16 \n",
            "b64            -           16       [32, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [32, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [32, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [32, 512, 16, 16]    float16 \n",
            "b32            -           16       [32, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [32, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [32, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [32, 512, 8, 8]      float32 \n",
            "b16            -           16       [32, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [32, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [32, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [32, 512, 4, 4]      float32 \n",
            "b8             -           16       [32, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [32, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [32, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [32, 512]            float32 \n",
            "b4.out         513         -        [32, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          21505025    2496480  -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2025-03-23 07:27:18.821444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742714838.855059    3648 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742714838.865891    3648 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-23 07:27:18.898237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Training for 5000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 08s       sec/tick 34.3    sec/kimg 1071.94 maintenance 33.9   cpumem 3.07   gpumem 12.66  augment 0.114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPUowsADSF2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}